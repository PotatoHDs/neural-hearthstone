{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH TO CARDS = ui\\cards\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "#\n",
    "# import rel\n",
    "# import websocket\n",
    "from PyQt6.QtGui import QFontDatabase\n",
    "# from hearthstone.enums import BlockType, Zone, Step, PlayState\n",
    "\n",
    "from Coach import Coach\n",
    "from Game import GameImp as Game\n",
    "from NN import NNetWrapper as nn\n",
    "# from fireplace.actions import Attack, Summon, Hit, EndTurn, Discover, Choice, MulliganChoice, Play, GenericChoice, \\\n",
    "#     BeginTurn, Death, TargetedAction, Activate\n",
    "# from fireplace.card import HeroPower, Hero, Character\n",
    "# from fireplace.exceptions import GameOver\n",
    "# from fireplace.managers import BaseObserver\n",
    "# from fireplace.player import Player\n",
    "from ui.ui import MainWindow\n",
    "from PyQt6.QtWidgets import *\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from observers import UiObserver, HsObserver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.n_in = 34 * 16\n",
    "        self.n_out = 21 * 18\n",
    "\n",
    "        self.epochs = 3\n",
    "        self.batch_size = 256\n",
    "        self.lr = 1e-4\n",
    "\n",
    "        self.arenaCompare = 1\n",
    "        self.numIters = 20\n",
    "        self.numEps = 13\n",
    "        self.maxlenOfQueue = 10000\n",
    "        self.numMCTS = 200\n",
    "        self.numItersForTrainExamplesHistory = 10\n",
    "\n",
    "        self.tempThreshold = 15\n",
    "        self.updateThreshold = 0.6\n",
    "\n",
    "        self.ngpu = 0\n",
    "        print(\"cuda:0\" if (torch.cuda.is_available() and self.ngpu > 0) else \"cpu\")\n",
    "        self.device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and self.ngpu > 0) else \"cpu\")\n",
    "\n",
    "        self.checkpoint = './temp/'\n",
    "        self.load_model = True\n",
    "        self.load_folder_file = ('./temp/', 'temp.pth.tar')\n",
    "        self.load_examples = ('./temp/', 'checkpoint.pth.tar')\n",
    "        self.fireplace_log_enabled = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Load trainExamples from file\n",
      "File with trainExamples found. Read it.\n",
      "------ITER 1------\n",
      "Checkpoint Directory exists! \n",
      "EPOCH ::: 1\n",
      "\u001B[KTraining Net |#                               | (1/17) Data: 0.003s | Batch: 1.527s | Total: 0:00:01 | ETA: 0:00:00 | Loss_pi: 5.9423 | Loss_v: 127.956\n",
      "\u001B[KTraining Net |###                             | (2/17) Data: 0.003s | Batch: 1.530s | Total: 0:00:03 | ETA: 0:00:25 | Loss_pi: 5.9922 | Loss_v: 169.782\n",
      "\u001B[KTraining Net |#####                           | (3/17) Data: 0.003s | Batch: 1.537s | Total: 0:00:04 | ETA: 0:00:23 | Loss_pi: 6.0069 | Loss_v: 206.519\n",
      "\u001B[KTraining Net |#######                         | (4/17) Data: 0.003s | Batch: 1.514s | Total: 0:00:06 | ETA: 0:00:22 | Loss_pi: 5.9903 | Loss_v: 221.889\n",
      "\u001B[KTraining Net |#########                       | (5/17) Data: 0.003s | Batch: 1.490s | Total: 0:00:07 | ETA: 0:00:20 | Loss_pi: 5.9241 | Loss_v: 234.311\n",
      "\u001B[KTraining Net |###########                     | (6/17) Data: 0.003s | Batch: 1.473s | Total: 0:00:08 | ETA: 0:00:18 | Loss_pi: 5.8213 | Loss_v: 228.593\n",
      "\u001B[KTraining Net |#############                   | (7/17) Data: 0.003s | Batch: 1.479s | Total: 0:00:10 | ETA: 0:00:17 | Loss_pi: 5.6582 | Loss_v: 229.651\n",
      "\u001B[KTraining Net |###############                 | (8/17) Data: 0.003s | Batch: 1.471s | Total: 0:00:11 | ETA: 0:00:15 | Loss_pi: 5.5473 | Loss_v: 236.944\n",
      "\u001B[KTraining Net |################                | (9/17) Data: 0.003s | Batch: 1.471s | Total: 0:00:13 | ETA: 0:00:14 | Loss_pi: 5.4306 | Loss_v: 240.395\n",
      "\u001B[KTraining Net |##################              | (10/17) Data: 0.003s | Batch: 1.467s | Total: 0:00:14 | ETA: 0:00:12 | Loss_pi: 5.3313 | Loss_v: 241.553\n",
      "\u001B[KTraining Net |####################            | (11/17) Data: 0.003s | Batch: 1.475s | Total: 0:00:16 | ETA: 0:00:11 | Loss_pi: 5.2129 | Loss_v: 243.230\n",
      "\u001B[KTraining Net |######################          | (12/17) Data: 0.003s | Batch: 1.472s | Total: 0:00:17 | ETA: 0:00:09 | Loss_pi: 5.1052 | Loss_v: 244.960\n",
      "\u001B[KTraining Net |########################        | (13/17) Data: 0.003s | Batch: 1.468s | Total: 0:00:19 | ETA: 0:00:08 | Loss_pi: 5.0010 | Loss_v: 246.425\n",
      "\u001B[KTraining Net |##########################      | (14/17) Data: 0.003s | Batch: 1.470s | Total: 0:00:20 | ETA: 0:00:06 | Loss_pi: 4.9180 | Loss_v: 247.109\n",
      "\u001B[KTraining Net |############################    | (15/17) Data: 0.003s | Batch: 1.470s | Total: 0:00:22 | ETA: 0:00:05 | Loss_pi: 4.8643 | Loss_v: 245.568\n",
      "\u001B[KTraining Net |##############################  | (16/17) Data: 0.003s | Batch: 1.469s | Total: 0:00:23 | ETA: 0:00:03 | Loss_pi: 4.8174 | Loss_v: 246.220\n",
      "\u001B[KTraining Net |################################| (17/17) Data: 0.003s | Batch: 1.464s | Total: 0:00:24 | ETA: 0:00:02 | Loss_pi: 4.7669 | Loss_v: 244.207\n",
      "\n",
      "\u001B[?25hEPOCH ::: 2\n",
      "\u001B[KTraining Net |#                               | (1/17) Data: 0.003s | Batch: 1.522s | Total: 0:00:01 | ETA: 0:00:00 | Loss_pi: 4.0520 | Loss_v: 252.000\n",
      "\u001B[KTraining Net |###                             | (2/17) Data: 0.003s | Batch: 1.475s | Total: 0:00:02 | ETA: 0:00:25 | Loss_pi: 4.0213 | Loss_v: 253.982\n",
      "\u001B[KTraining Net |#####                           | (3/17) Data: 0.003s | Batch: 1.504s | Total: 0:00:04 | ETA: 0:00:23 | Loss_pi: 4.0431 | Loss_v: 251.987\n",
      "\u001B[KTraining Net |#######                         | (4/17) Data: 0.003s | Batch: 1.489s | Total: 0:00:05 | ETA: 0:00:22 | Loss_pi: 4.0022 | Loss_v: 255.986\n",
      "\u001B[KTraining Net |#########                       | (5/17) Data: 0.003s | Batch: 1.489s | Total: 0:00:07 | ETA: 0:00:20 | Loss_pi: 4.0022 | Loss_v: 256.780\n",
      "\u001B[KTraining Net |###########                     | (6/17) Data: 0.003s | Batch: 1.465s | Total: 0:00:08 | ETA: 0:00:18 | Loss_pi: 3.9739 | Loss_v: 254.665\n",
      "\u001B[KTraining Net |#############                   | (7/17) Data: 0.003s | Batch: 1.456s | Total: 0:00:10 | ETA: 0:00:17 | Loss_pi: 3.9458 | Loss_v: 253.035\n",
      "\u001B[KTraining Net |###############                 | (8/17) Data: 0.003s | Batch: 1.459s | Total: 0:00:11 | ETA: 0:00:15 | Loss_pi: 3.9616 | Loss_v: 253.551\n",
      "\u001B[KTraining Net |################                | (9/17) Data: 0.003s | Batch: 1.464s | Total: 0:00:13 | ETA: 0:00:14 | Loss_pi: 4.0711 | Loss_v: 255.275\n",
      "\u001B[KTraining Net |##################              | (10/17) Data: 0.003s | Batch: 1.465s | Total: 0:00:14 | ETA: 0:00:12 | Loss_pi: 4.6522 | Loss_v: 253.689\n",
      "\u001B[KTraining Net |####################            | (11/17) Data: 0.003s | Batch: 1.463s | Total: 0:00:16 | ETA: 0:00:11 | Loss_pi: 5.3628 | Loss_v: 252.081\n",
      "\u001B[KTraining Net |######################          | (12/17) Data: 0.003s | Batch: 1.466s | Total: 0:00:17 | ETA: 0:00:09 | Loss_pi: 5.9427 | Loss_v: 252.074\n",
      "\u001B[KTraining Net |########################        | (13/17) Data: 0.003s | Batch: 1.462s | Total: 0:00:19 | ETA: 0:00:08 | Loss_pi: 6.3622 | Loss_v: 252.683\n",
      "\u001B[KTraining Net |##########################      | (14/17) Data: 0.003s | Batch: 1.463s | Total: 0:00:20 | ETA: 0:00:06 | Loss_pi: 6.5597 | Loss_v: 252.622\n",
      "\u001B[KTraining Net |############################    | (15/17) Data: 0.003s | Batch: 1.460s | Total: 0:00:21 | ETA: 0:00:05 | Loss_pi: 6.5799 | Loss_v: 250.713\n",
      "\u001B[KTraining Net |##############################  | (16/17) Data: 0.003s | Batch: 1.458s | Total: 0:00:23 | ETA: 0:00:03 | Loss_pi: 6.4424 | Loss_v: 250.229\n",
      "\u001B[KTraining Net |################################| (17/17) Data: 0.003s | Batch: 1.459s | Total: 0:00:24 | ETA: 0:00:02 | Loss_pi: 6.3213 | Loss_v: 251.152\n",
      "\n",
      "\u001B[?25hEPOCH ::: 3\n",
      "\u001B[KTraining Net |#                               | (1/17) Data: 0.002s | Batch: 1.490s | Total: 0:00:01 | ETA: 0:00:00 | Loss_pi: 4.0899 | Loss_v: 219.707\n",
      "\u001B[KTraining Net |###                             | (2/17) Data: 0.003s | Batch: 1.484s | Total: 0:00:02 | ETA: 0:00:24 | Loss_pi: 4.1096 | Loss_v: 229.131\n",
      "\u001B[KTraining Net |#####                           | (3/17) Data: 0.003s | Batch: 1.483s | Total: 0:00:04 | ETA: 0:00:23 | Loss_pi: 4.2087 | Loss_v: 241.335\n",
      "\u001B[KTraining Net |#######                         | (4/17) Data: 0.003s | Batch: 1.512s | Total: 0:00:06 | ETA: 0:00:21 | Loss_pi: 4.1483 | Loss_v: 240.384\n",
      "\u001B[KTraining Net |#########                       | (5/17) Data: 0.003s | Batch: 1.531s | Total: 0:00:07 | ETA: 0:00:20 | Loss_pi: 4.1195 | Loss_v: 248.310\n",
      "\u001B[KTraining Net |###########                     | (6/17) Data: 0.003s | Batch: 1.509s | Total: 0:00:09 | ETA: 0:00:19 | Loss_pi: 4.1264 | Loss_v: 248.648\n",
      "\u001B[KTraining Net |#############                   | (7/17) Data: 0.003s | Batch: 1.502s | Total: 0:00:10 | ETA: 0:00:17 | Loss_pi: 4.1604 | Loss_v: 248.063\n",
      "\u001B[KTraining Net |###############                 | (8/17) Data: 0.003s | Batch: 1.490s | Total: 0:00:11 | ETA: 0:00:16 | Loss_pi: 4.2683 | Loss_v: 248.730\n",
      "\u001B[KTraining Net |################                | (9/17) Data: 0.003s | Batch: 1.489s | Total: 0:00:13 | ETA: 0:00:14 | Loss_pi: 4.3544 | Loss_v: 246.490\n",
      "\u001B[KTraining Net |##################              | (10/17) Data: 0.003s | Batch: 1.488s | Total: 0:00:14 | ETA: 0:00:12 | Loss_pi: 4.4280 | Loss_v: 248.262\n",
      "\u001B[KTraining Net |####################            | (11/17) Data: 0.003s | Batch: 1.480s | Total: 0:00:16 | ETA: 0:00:11 | Loss_pi: 4.4614 | Loss_v: 245.500\n",
      "\u001B[KTraining Net |######################          | (12/17) Data: 0.003s | Batch: 1.480s | Total: 0:00:17 | ETA: 0:00:09 | Loss_pi: 4.4688 | Loss_v: 247.181\n",
      "\u001B[KTraining Net |########################        | (13/17) Data: 0.003s | Batch: 1.474s | Total: 0:00:19 | ETA: 0:00:08 | Loss_pi: 4.4533 | Loss_v: 246.425\n",
      "\u001B[KTraining Net |##########################      | (14/17) Data: 0.003s | Batch: 1.473s | Total: 0:00:20 | ETA: 0:00:06 | Loss_pi: 4.4508 | Loss_v: 246.482\n",
      "\u001B[KTraining Net |############################    | (15/17) Data: 0.003s | Batch: 1.471s | Total: 0:00:22 | ETA: 0:00:05 | Loss_pi: 4.4287 | Loss_v: 245.814\n",
      "\u001B[KTraining Net |##############################  | (16/17) Data: 0.003s | Batch: 1.469s | Total: 0:00:23 | ETA: 0:00:03 | Loss_pi: 4.4182 | Loss_v: 243.951\n",
      "\u001B[KTraining Net |################################| (17/17) Data: 0.003s | Batch: 1.464s | Total: 0:00:24 | ETA: 0:00:02 | Loss_pi: 4.4057 | Loss_v: 242.158\n",
      "\n",
      "\u001B[?25hTest game started\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n",
      "Turn happened1\n",
      "Turn happened2\n",
      "Turn happened3\n",
      "Turn happened4\n",
      "Turn happened\n"
     ]
    }
   ],
   "source": [
    "args = Args()\n",
    "g = Game()\n",
    "nnet = nn(args)\n",
    "\n",
    "if args.load_model:\n",
    "    nnet.load_checkpoint(args.load_folder_file[0], args.load_folder_file[1])\n",
    "\n",
    "c = Coach(g, nnet, args)\n",
    "if args.load_model:\n",
    "    print(\"Load trainExamples from file\")\n",
    "    c.load_train_examples()\n",
    "c.learn()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}